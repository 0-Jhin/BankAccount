spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: mistral
          temperature: 0.7
        enabled: true

langchain4j:
  ollama:
    chat-model:
      model-name: llama3
      log-requests: true
      log-responses: true
      base-url:
